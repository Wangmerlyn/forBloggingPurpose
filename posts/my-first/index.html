<!doctype html><html lang=en><head><title>APF review :: Terminal</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="&amp;ldquo;Communication-Efficient Federated Learning with Adaptive Parameter Freezing&amp;rdquo; review Back Ground There has been a great amount of communication cost in federated learning thus communication has become a bottleneck in Federated learning.
Way to reduce the communication amount for distributed model training quantizing the update into fewer bits sparsify local updates The Challenges of boosting federal learning efficiency by skipping out of stablized parameters How to identify stabilized parameters effectively How to ensure that the model will converge to the optimal state The parameter changing feature By assuming the global loss funcition of deep neural network is a $ \mu $ -strong convex function and the stochastic gradient of the loss funcition is bounded, the authors drew the conclusion that in the earlier stage of deep neural network training, the parameters $ \omega $ approaches the optimal state $ \omega^{*} $ exponentially fast while in the later iterations, $ \omega $ tend to oscillate around $ \omega^* $."><meta name=keywords content="freezing parameter,"><meta name=robots content="noodp"><link rel=canonical href=https://wangmerlyn.github.io/posts/my-first/><link rel=stylesheet href=https://wangmerlyn.github.io/assets/style.css><link rel=stylesheet href=https://wangmerlyn.github.io/assets/pink.css><link rel=apple-touch-icon href=https://wangmerlyn.github.io/img/apple-touch-icon-192x192.png><link rel="shortcut icon" href=https://wangmerlyn.github.io/img/favicon/pink.png><meta name=twitter:card content="summary"><meta name=twitter:site content><meta name=twitter:creator content><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="APF review"><meta property="og:description" content="&amp;ldquo;Communication-Efficient Federated Learning with Adaptive Parameter Freezing&amp;rdquo; review Back Ground There has been a great amount of communication cost in federated learning thus communication has become a bottleneck in Federated learning.
Way to reduce the communication amount for distributed model training quantizing the update into fewer bits sparsify local updates The Challenges of boosting federal learning efficiency by skipping out of stablized parameters How to identify stabilized parameters effectively How to ensure that the model will converge to the optimal state The parameter changing feature By assuming the global loss funcition of deep neural network is a $ \mu $ -strong convex function and the stochastic gradient of the loss funcition is bounded, the authors drew the conclusion that in the earlier stage of deep neural network training, the parameters $ \omega $ approaches the optimal state $ \omega^{*} $ exponentially fast while in the later iterations, $ \omega $ tend to oscillate around $ \omega^* $."><meta property="og:url" content="https://wangmerlyn.github.io/posts/my-first/"><meta property="og:site_name" content="Terminal"><meta property="og:image" content="https://wangmerlyn.github.io/"><meta property="og:image:width" content="2048"><meta property="og:image:height" content="1024"><meta property="article:published_time" content="2022-07-26 16:09:39 +0800 +0800"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script></head><body class=pink><div class="container headings--one-size"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>Terminal</div></a></div><div class=menu-trigger>menu</div></div><nav class=menu><ul class="menu__inner menu__inner--desktop"><li><a href=/about>About</a></li><li><a href=/showcase>Showcase</a></li></ul><ul class="menu__inner menu__inner--mobile"><li><a href=/about>About</a></li><li><a href=/showcase>Showcase</a></li></ul></nav></header><div class=content><div class=post><h1 class=post-title><a href=https://wangmerlyn.github.io/posts/my-first/>APF review</a></h1><div class=post-meta><span class=post-date>2022-07-26</span>
<span class=post-author>:: Wangmerlyn</span></div><span class=post-tags>#<a href=https://wangmerlyn.github.io/tags/distributed-learning/>distributed learning</a>&nbsp;
#<a href=https://wangmerlyn.github.io/tags/ai-system/>AI system</a>&nbsp;</span><div class=post-content><div><h1 id=communication-efficient-federated-learning-with-adaptive-parameter-freezing-review>&ldquo;Communication-Efficient Federated Learning with Adaptive Parameter Freezing&rdquo; review<a href=#communication-efficient-federated-learning-with-adaptive-parameter-freezing-review class=hanchor arialabel=Anchor>&#8983;</a></h1><h2 id=back-ground>Back Ground<a href=#back-ground class=hanchor arialabel=Anchor>&#8983;</a></h2><p>There has been a great amount of communication cost in federated learning thus communication has become a bottleneck in Federated learning.</p><h3 id=way-to-reduce-the-communication-amount-for-distributed-model-training>Way to reduce the communication amount for distributed model training<a href=#way-to-reduce-the-communication-amount-for-distributed-model-training class=hanchor arialabel=Anchor>&#8983;</a></h3><ul><li><em>quantizing</em> the update into fewer bits</li><li><em>sparsify</em> local updates</li></ul><h3 id=the-challenges-of-boosting-federal-learning-efficiency-by-skipping-out-of-stablized-parameters>The Challenges of boosting federal learning efficiency by skipping out of stablized parameters<a href=#the-challenges-of-boosting-federal-learning-efficiency-by-skipping-out-of-stablized-parameters class=hanchor arialabel=Anchor>&#8983;</a></h3><ul><li>How to identify stabilized parameters effectively</li><li>How to ensure that the model will converge to the optimal state</li></ul><h2 id=the-parameter-changing-feature>The parameter changing feature<a href=#the-parameter-changing-feature class=hanchor arialabel=Anchor>&#8983;</a></h2><p>By assuming the global loss funcition of deep neural network is a $ \mu $ -strong convex function and the stochastic gradient of the loss funcition is bounded, the authors drew the conclusion that in the earlier stage of deep neural network training, the parameters $ \omega $ approaches the optimal state $ \omega^{*} $ exponentially fast while in the later iterations, $ \omega $ tend to oscillate around $ \omega^* $.</p><h2 id=identify-stablized-parameters>Identify stablized parameters<a href=#identify-stablized-parameters class=hanchor arialabel=Anchor>&#8983;</a></h2><p>At first the authors proposed a indicator called <em>effective perbutation</em> $$P_K^r=\frac{||\sum_{i=0}^{r} \mu_{k-i}||}{\sum_{i=0}^{r}||\mu_{k-i}||}$$ to describe the stableness of a certain parameter. Later the indicator was improved using exponential average $P_K=\frac{E_K}{E_K^{abs}}$ where $E_K = \alpha E_{K-1}+(1-\alpha)\Delta_K$, $E_{K}^{abs}=\alpha E_{K-1}+(1-\alpha)|\Delta_K|$ to avoid wasting storage for model snapshots and adjust the focus on recent model updates. If the effective perbutation hits below a threshold, the parameter is considered stable. The threshold can be adaptively changing.</p><h2 id=adaptive-parameter-freezing>Adaptive parameter freezing<a href=#adaptive-parameter-freezing class=hanchor arialabel=Anchor>&#8983;</a></h2><h3 id=partial-syncing>Partial syncing<a href=#partial-syncing class=hanchor arialabel=Anchor>&#8983;</a></h3><p>Freezing parameters locally on individual worker has been proven to be working poorly since works deal with non IDD data in a federated learning task, causing parameters to diverge after previous stablized parameters are out of sync.</p><h3 id=permanent-freezing>Permanent freezing<a href=#permanent-freezing class=hanchor arialabel=Anchor>&#8983;</a></h3><p>Permanent parameter freezing is gonna result in the loss of acurracy since parameters that are freezed when they are stable halfway in the training are still probably not optimal.</p><h3 id=parameter-freezing-principles>Parameter freezing principles<a href=#parameter-freezing-principles class=hanchor arialabel=Anchor>&#8983;</a></h3><p>From the 2 observations above, the authors drew the following principles for adaptive parameter freezing.</p><ol><li>Stablized parameters must be kept unchanged on each worker.</li><li>Temporary stableness must be handled.</li></ol><h3 id=apf-method>APF method<a href=#apf-method class=hanchor arialabel=Anchor>&#8983;</a></h3><p>With the 2 principles above combined, the authors raised an &ldquo;adaptive parameter freezing&rdquo; mechanism. While keeping an eye on those stablized parameters, the freezing period should also be adaptively changing. After a parameter is recognized as stabilized, it should freezed for a specific amount of time that is controlled by a adaptive mechanism which is similar to TCP AIMD. By checking whether the parameter remains stable after unfreezing, the mechanism changes the freezing period adaptively. Taking efficiency into consideration, effective perbutation check is not necessary to be conducted once after each iteration,</p><h3 id=agressive-apf>Agressive APF<a href=#agressive-apf class=hanchor arialabel=Anchor>&#8983;</a></h3><p>While dealing with over-parameterized models, certain parameters may not tend to converge due to their non-convex nature. Agressive APF works like APF but also randomly freeze parameters therefore filter out the training of less needed parameters.</p></div></div></div></div><footer class=footer><div class=footer__inner><div class=copyright><span>Â© 2022 Powered by <a href=http://gohugo.io>Hugo</a></span>
<span>:: Theme made by <a href=https://twitter.com/panr>panr</a></span></div></div></footer><script src=https://wangmerlyn.github.io/assets/main.js></script>
<script src=https://wangmerlyn.github.io/assets/prism.js></script></div></body></html>